{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Thanks for taking part in our competition and downloading the starting kit! We've compiled this readme to show\n",
    "you how to run all the scripts that we'll use to evaluate your submissions, as well as give you a quick tour of what\n",
    "we've included. Please feel free to reach out to use for help or clarifications, using the contact details on the\n",
    "competition page. Additionally, check out the CodaLab wiki here: https://github.com/codalab/codalab-competitions/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this Notebook\n",
    "This notebook walks through writing a sample submission, and then takes that submission through our evaluation pipeline. Feel free to replace this sample submission with your own, so that you can check to make sure your submission works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our NAS class\n",
    "Every submission must contain a `NAS` class. This class must contain a method `search` that receives input data and returns a valid PyTorch model for that data. We've written a very simple sample, one that modifies a ResNet18 to compatible with the dataset and task. This class is all you need\n",
    "to provide for your submission, so don't worry about writing things data loading functions, all of the will be automatically handled once you submit. This sample submission is the \"benchmark\" submission; every submission will scored based on how it compares to this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class NAS:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # given some input data, return the \"best possible\" architecture\n",
    "    def search(self, train_x, train_y, valid_x, valid_y, metadata):\n",
    "        n_classes = metadata['n_classes']\n",
    "\n",
    "        # load resnet18 model (definitely not the best possible architecture, but it'll work as an example!)\n",
    "        model = torchvision.models.resnet50()\n",
    "\n",
    "        # reshape it to this dataset\n",
    "        model.conv1 = nn.Conv2d(train_x.shape[1], 64, kernel_size=(7, 7), stride=1, padding=3)\n",
    "        model.fc = nn.Linear(model.fc.in_features, n_classes, bias=True)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit, all you need to do is zip together a `nas.py` file that contains your NAS class, any helper scripts you might need, and the `metadata` file. Make sure to set the `full_training` flag accordingly in the metadata file, depending on whether you want to run the full training pipeline on our servers or just a quick debug pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "When you submit, data loading will be handled for you, all you need to supply is the `NAS` class. However, for debugging it can be helpful to load the data manually, so here's how to do it.\n",
    "\n",
    "## Data Format\n",
    "When we load a dataset, we'll load five numpy arrays and the dataset metadata file.\n",
    "    \n",
    "The five arrays are as follows:\n",
    "    * train_x: numpy array of shape (n_datapoints, channels, weight, height). This is our input training data to the model. Each dataset will comprise of images. All images within a dataset are of identical channel size and spatial size, however these will vary *between* datasets. These datapoints\n",
    "    are pre-normalized and shuffled; there should be no need to perform data augmentation over them. These datapoints are exactly identical to the ones that will used to train the models found by your algorithm.\n",
    "\t* train_y: numpy array of shape (n_datapoints). These are our training labels. It's an array of integers, such that train_y[i] corresponds to the label of train_x[i]. \n",
    "    * valid_x: numpy array of shape (n_datapoints, channels, weight, height). Our input validation data. It'll be of the exact same shape of the training input data.\n",
    "    * valid_y: numpy array of shape (n_datapoints). Our validation labels, again an array of integers.\n",
    "    * test_x: numpy array of shape (n_datapoints, channels, weight, height). Our input test data. It'll be of the exact same shape of the training input data.\n",
    "    \n",
    "    \n",
    "The metadata is a dictionary that contains the following keys:\n",
    "    * batch_size: the batch size that will be used to train this da==taset\n",
    "    * n_classes: the total number of classes in the classification task\n",
    "    * lr: the learning rate that will be used to train this dataset\n",
    "    * benchmark: the threshold used to determine zero point of scoring; your score on the dataset will equal '10 * (test_acc - benchmark) / (100-test_acc)'. \n",
    "        - This means you can score a maximum of 10 points on each dataset: a full 10 points will be awarded for 100% test accuracy, while 0 points will be awarded for a test accuracy equal to the benchmark. \n",
    "    * name: a unique name for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sample_dataset_0 ==================================================\n",
      "Train X shape: (100, 3, 28, 28)\n",
      "Train Y shape: (100,)\n",
      "Valid X shape: (100, 3, 28, 28)\n",
      "Valid Y shape: (100,)\n",
      "Test X shape: (100, 3, 28, 28)\n",
      "Metadata: {'batch_size': 64, 'n_classes': 20, 'lr': 0.01, 'benchmark': 92.08, 'name': 'sample_dataset_0'}\n",
      "===== EVALUATING sample_dataset_0 =====\n",
      "Cuda available? True\n",
      "=== EPOCH 0 ===\n",
      "  Train Acc:     6.000%, Val Acc:    7.000%, Mem Alloc:  722.00MiB, T Remaining Est: 1.37s\n",
      "  Train Loss:    3.260 , Val Loss:    3.001\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "=== EPOCH 1 ===\n",
      "  Train Acc:     4.000%, Val Acc:    4.000%, Mem Alloc:  722.00MiB, T Remaining Est: 1.13s\n",
      "  Train Loss:    3.231 , Val Loss:    3.009\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "=== EPOCH 2 ===\n",
      "  Train Acc:     9.000%, Val Acc:    3.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.95s\n",
      "  Train Loss:    3.629 , Val Loss:    3.446\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "=== EPOCH 3 ===\n",
      "  Train Acc:    17.000%, Val Acc:    5.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.76s\n",
      "  Train Loss:    2.827 , Val Loss:    6.944\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "=== EPOCH 4 ===\n",
      "  Train Acc:    35.000%, Val Acc:    7.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.57s\n",
      "  Train Loss:    2.404 , Val Loss:    6.801\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "=== EPOCH 5 ===\n",
      "  Train Acc:    64.000%, Val Acc:    7.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.38s\n",
      "  Train Loss:    1.737 , Val Loss:    4.380\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "=== EPOCH 6 ===\n",
      "  Train Acc:    80.000%, Val Acc:    3.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.19s\n",
      "  Train Loss:    1.336 , Val Loss:    3.505\n",
      "  Current best score:    Val Acc:     7.000% @ epoch 0\n",
      "\n",
      "=== sample_dataset_1 ==================================================\n",
      "Train X shape: (100, 1, 28, 28)\n",
      "Train Y shape: (100,)\n",
      "Valid X shape: (100, 1, 28, 28)\n",
      "Valid Y shape: (100,)\n",
      "Test X shape: (100, 1, 28, 28)\n",
      "Metadata: {'batch_size': 64, 'n_classes': 10, 'lr': 0.01, 'benchmark': 92.87, 'name': 'sample_dataset_1'}\n",
      "===== EVALUATING sample_dataset_1 =====\n",
      "Cuda available? True\n",
      "=== EPOCH 0 ===\n",
      "  Train Acc:     6.000%, Val Acc:    9.000%, Mem Alloc:  722.00MiB, T Remaining Est: 1.40s\n",
      "  Train Loss:    2.568 , Val Loss:    2.294\n",
      "  Current best score:    Val Acc:     9.000% @ epoch 0\n",
      "=== EPOCH 1 ===\n",
      "  Train Acc:    15.000%, Val Acc:    6.000%, Mem Alloc:  722.00MiB, T Remaining Est: 1.18s\n",
      "  Train Loss:    4.321 , Val Loss:    2.362\n",
      "  Current best score:    Val Acc:     9.000% @ epoch 0\n",
      "=== EPOCH 2 ===\n",
      "  Train Acc:    11.000%, Val Acc:    5.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.96s\n",
      "  Train Loss:    4.122 , Val Loss:    2.343\n",
      "  Current best score:    Val Acc:     9.000% @ epoch 0\n",
      "=== EPOCH 3 ===\n",
      "  Train Acc:    29.000%, Val Acc:    7.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.76s\n",
      "  Train Loss:    2.284 , Val Loss:    2.425\n",
      "  Current best score:    Val Acc:     9.000% @ epoch 0\n",
      "=== EPOCH 4 ===\n",
      "  Train Acc:    39.000%, Val Acc:    6.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.57s\n",
      "  Train Loss:    2.383 , Val Loss:    2.455\n",
      "  Current best score:    Val Acc:     9.000% @ epoch 0\n",
      "=== EPOCH 5 ===\n",
      "  Train Acc:    57.000%, Val Acc:    6.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.39s\n",
      "  Train Loss:    1.797 , Val Loss:    2.389\n",
      "  Current best score:    Val Acc:     9.000% @ epoch 0\n",
      "=== EPOCH 6 ===\n",
      "  Train Acc:    66.000%, Val Acc:   11.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.19s\n",
      "  Train Loss:    1.188 , Val Loss:    2.379\n",
      "  Current best score:    Val Acc:    11.000% @ epoch 0\n",
      "\n",
      "=== sample_dataset_2 ==================================================\n",
      "Train X shape: (100, 1, 24, 24)\n",
      "Train Y shape: (100,)\n",
      "Valid X shape: (100, 1, 24, 24)\n",
      "Valid Y shape: (100,)\n",
      "Test X shape: (100, 1, 24, 24)\n",
      "Metadata: {'batch_size': 64, 'n_classes': 10, 'lr': 0.01, 'benchmark': 87.0, 'name': 'sample_dataset_2'}\n",
      "===== EVALUATING sample_dataset_2 =====\n",
      "Cuda available? True\n",
      "=== EPOCH 0 ===\n",
      "  Train Acc:     8.000%, Val Acc:   10.000%, Mem Alloc:  722.00MiB, T Remaining Est: 1.22s\n",
      "  Train Loss:    2.706 , Val Loss:    2.319\n",
      "  Current best score:    Val Acc:    10.000% @ epoch 0\n",
      "=== EPOCH 1 ===\n",
      "  Train Acc:    14.000%, Val Acc:    8.000%, Mem Alloc:  722.00MiB, T Remaining Est: 1.07s\n",
      "  Train Loss:    3.499 , Val Loss:    2.327\n",
      "  Current best score:    Val Acc:    10.000% @ epoch 0\n",
      "=== EPOCH 2 ===\n",
      "  Train Acc:     8.000%, Val Acc:   16.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.92s\n",
      "  Train Loss:    3.538 , Val Loss:    2.291\n",
      "  Current best score:    Val Acc:    16.000% @ epoch 0\n",
      "=== EPOCH 3 ===\n",
      "  Train Acc:    14.000%, Val Acc:   12.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.73s\n",
      "  Train Loss:    3.094 , Val Loss:    2.378\n",
      "  Current best score:    Val Acc:    16.000% @ epoch 0\n",
      "=== EPOCH 4 ===\n",
      "  Train Acc:    35.000%, Val Acc:    5.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.54s\n",
      "  Train Loss:    2.360 , Val Loss:    2.463\n",
      "  Current best score:    Val Acc:    16.000% @ epoch 0\n",
      "=== EPOCH 5 ===\n",
      "  Train Acc:    33.000%, Val Acc:    5.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.36s\n",
      "  Train Loss:    1.591 , Val Loss:    2.420\n",
      "  Current best score:    Val Acc:    16.000% @ epoch 0\n",
      "=== EPOCH 6 ===\n",
      "  Train Acc:    64.000%, Val Acc:    5.000%, Mem Alloc:  722.00MiB, T Remaining Est: 0.18s\n",
      "  Train Loss:    1.168 , Val Loss:    2.416\n",
      "  Current best score:    Val Acc:    16.000% @ epoch 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the exact data loaders that we'll use to load the data\n",
    "from ingestion_program.nascomp.helpers import *\n",
    "\n",
    "# load the exact retraining script we'll use to evaluate the found models\n",
    "from ingestion_program.nascomp.torch_evaluator import *\n",
    "\n",
    "# if you want to use the real development data, download the public data and set data_dir appropriately\n",
    "data_dir = 'sample_data'\n",
    "\n",
    "\n",
    "# find all the datasets in the given directory:\n",
    "dataset_paths = get_dataset_paths(data_dir)\n",
    "dataset_predictions = []\n",
    "for path in dataset_paths:\n",
    "    (train_x, train_y), (valid_x, valid_y), (test_x), metadata = load_datasets(path)\n",
    "    print(\"=== {} {}\".format(metadata['name'],\"=\"*50))\n",
    "    print(\"Train X shape:\",train_x.shape)\n",
    "    print(\"Train Y shape:\",train_y.shape)\n",
    "    print(\"Valid X shape:\",valid_x.shape)\n",
    "    print(\"Valid Y shape:\",valid_y.shape)\n",
    "    print(\"Test X shape:\", test_x.shape)\n",
    "    print(\"Metadata:\", metadata)\n",
    "    \n",
    "\n",
    "    # initialize our NAS class\n",
    "    nas = NAS()\n",
    "    \n",
    "    # search for a model\n",
    "    model = nas.search(train_x, train_y, valid_x, valid_y, metadata)\n",
    "    \n",
    "    # package data for the evaluator\n",
    "    data = (train_x, train_y), (valid_x, valid_y), test_x\n",
    "    \n",
    "    # retrain the model from scratch\n",
    "    results = torch_evaluator(model, data, metadata, n_epochs=7, full_train=True)\n",
    "    \n",
    "    # clean up the NAS class\n",
    "    del nas\n",
    "    \n",
    "    # save our predictions\n",
    "    dataset_predictions.append(results['test_predictions'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the Predictions\n",
    "Again this will be all handled for you upon submission, but here's a copy of the scoring script so you can test things locally. We first load the labels for each test dataset, and then compare the accuracy of your model's predictions against these test labels. This score is then\n",
    "adjusted according to the score benchmark. The scores will be pretty terrible over the sample data; download the public data and use that to get an accurate picture of performance over the Development Phase data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scoring sample_dataset_0 ===\n",
      "  Raw score: 4.0\n",
      "  Benchmark: 92.08\n",
      "  Adjusted:   -111.21212121212119\n",
      "=== Scoring sample_dataset_1 ===\n",
      "  Raw score: 7.000000000000001\n",
      "  Benchmark: 92.87\n",
      "  Adjusted:   -120.43478260869573\n",
      "=== Scoring sample_dataset_2 ===\n",
      "  Raw score: 9.0\n",
      "  Benchmark: 87.0\n",
      "  Adjusted:   -60.0\n",
      "['Dataset_0_Score: -111.212', 'Dataset_1_Score: -120.435', 'Dataset_2_Score: -60.000', 'Overall_Score: -291.647']\n"
     ]
    }
   ],
   "source": [
    "overall_score = 0\n",
    "out = []\n",
    "for i, path in enumerate(dataset_paths):\n",
    "\n",
    "    # load the reference values\n",
    "    ref_y = np.load(os.path.join(path, 'test_y.npy'))\n",
    "\n",
    "    # load the dataset_metadata for this dataset\n",
    "    metadata =  load_dataset_metadata(path)\n",
    "    \n",
    "    print(\"=== Scoring {} ===\".format(metadata['name']))\n",
    "    index = metadata['name'][-1]\n",
    "\n",
    "    # load the model predictions\n",
    "    pred_y = dataset_predictions[i]\n",
    "\n",
    "    # compute accuracy\n",
    "    score = sum(ref_y == pred_y)/float(len(ref_y)) * 100\n",
    "    print(\"  Raw score:\", score)\n",
    "    print(\"  Benchmark:\", metadata['benchmark'])\n",
    "\n",
    "    # adjust score according to benchmark\n",
    "    point_weighting = 10/(100 - metadata['benchmark'])\n",
    "    score -= metadata['benchmark']\n",
    "    score *= point_weighting\n",
    "    print(\"  Adjusted:  \", score)\n",
    "\n",
    "    # add per-dataset score to overall\n",
    "    overall_score += score\n",
    "\n",
    "    # add to scoring stringg\n",
    "    out.append(\"Dataset_{}_Score: {:.3f}\".format(index, score))\n",
    "out.append(\"Overall_Score: {:.3f}\".format(overall_score))\n",
    "\n",
    "# print score\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
